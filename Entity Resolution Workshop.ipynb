{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Workshop\n",
    "\n",
    "Entity Resolution is the task of disambiguating manifestations of real world entities through linking and grouping and is often an essential part of the data wrangling process. There are three primary tasks involved in entity resolution: deduplication, record linkage, and canonicalization; each of which serve to improve data quality by reducing irrelevant or repeated data, joining information from disparate records, and providing a single source of information to perform analytics upon. However, due to data quality issues (misspellings or incorrect data), schema variations in different sources, or simply different representations, entity resolution is not a straightforward process and most ER techniques utilize machine learning and other stochastic approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import distance \n",
    "\n",
    "## Important Paths\n",
    "FIXTURES = os.path.join(os.getcwd(), \"fixtures\")\n",
    "PRODUCTS = os.path.join(FIXTURES, \"products\")\n",
    "\n",
    "## Module Constants\n",
    "GOOGID   = 'http://www.google.com/base/feeds/snippets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    \"\"\"\n",
    "    Create a generator to load data from the products data source.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(PRODUCTS, name), 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def google_key(key):\n",
    "    return os.path.join(GOOGID, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load Datasets into Memory\n",
    "amazon  = list(load_data('amazon.csv'))\n",
    "google  = list(load_data('google.csv'))\n",
    "mapping = list(load_data('perfect_mapping.csv'))\n",
    "\n",
    "## Report on contents of the dataset\n",
    "for name, dataset in (('Amazon', amazon), ('Google Shopping', google)):\n",
    "    print \"{} dataset contains {} records\".format(name, len(dataset))\n",
    "    print \"Record keys: {}\\n\".format(\", \".join(dataset[0].keys()))\n",
    "\n",
    "## Report on the contents of the mapping\n",
    "print \"There are {} matching records to link\".format(len(mapping))\n",
    "\n",
    "## Convert dataset to records indexed by their ID.\n",
    "amazon  = dict((v['id'], v) for v in amazon)\n",
    "google  = dict((v['id'], v) for v in google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = amazon['b0000c7fpt']\n",
    "Y = google[google_key('17175991674191849246')]\n",
    "\n",
    "## Show example Records\n",
    "print json.dumps(X, indent=2)\n",
    "print json.dumps(Y, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to information about distance metrics:\n",
    "\n",
    "- [Implementing the Five Most Popular Similarity Measures in Python](http://dataconomy.com/implementing-the-five-most-popular-similarity-measures-in-python/)\n",
    "- [Scikit-Learn Distance Metric](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)\n",
    "- [Python Distance Library](https://pypi.python.org/pypi/Distance/)\n",
    "\n",
    "Numeric distances are fairly easy, but can be record specific (e.g. phone numbers can compare area codes, city codes, etc. to determine similarity). We will compare text similarity in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Typographic Distances\n",
    "\n",
    "print distance.levenshtein(\"lenvestein\", \"levenshtein\")\n",
    "print distance.hamming(\"hamming\", \"hamning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare glyphs, syllables, or phonemes \n",
    "t1 = (\"de\", \"ci\", \"si\", \"ve\")\n",
    "t2 = (\"de\", \"ri\", \"si\", \"ve\")\n",
    "print distance.levenshtein(t1, t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sentence Comparison\n",
    "sent1 = \"The quick brown fox jumped over the lazy dogs.\"\n",
    "sent2 = \"The lazy foxes are jumping over the crazy Dog.\"\n",
    "\n",
    "print distance.nlevenshtein(sent1.split(), sent2.split(), method=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "print distance.hamming(\"fat\", \"cat\", normalized=True)\n",
    "print distance.nlevenshtein(\"abc\", \"acd\", method=1)  # shortest alignment\n",
    "print distance.nlevenshtein(\"abc\", \"acd\", method=2)  # longest alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set measures\n",
    "print distance.sorensen(\"decide\", \"resize\")\n",
    "print distance.jaccard(\"decide\", \"resize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Text Score\n",
    "\n",
    "Use text preprocessing with NLTK to split long strings into parts, and normalize them using Wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    \"\"\"\n",
    "    When passed in a sentence, tokenizes and normalizes the string,\n",
    "    returning a list of lemmata.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.WordNetLemmatizer() \n",
    "    for token in nltk.wordpunct_tokenize(sent):\n",
    "        token = token.lower()\n",
    "        yield lemmatizer.lemmatize(token)\n",
    "\n",
    "def normalized_jaccard(*args):\n",
    "    try:\n",
    "        return distance.jaccard(*[tokenize(arg) for arg in args])\n",
    "    except UnicodeDecodeError:\n",
    "        return 0.0\n",
    "\n",
    "print normalized_jaccard(sent1, sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity(prod1, prod2):\n",
    "    \"\"\"\n",
    "    Returns a similarity vector of match scores:\n",
    "    [name_score, description_score, manufacturer_score, price_score]\n",
    "    \"\"\"\n",
    "    pair  = (prod1, prod2)\n",
    "    names = [r.get('name', None) or r.get('title', None) for r in pair]\n",
    "    descr = [r.get('description') for r in pair]\n",
    "    manuf = [r.get('manufacturer') for r in pair]\n",
    "    price = [float(r.get('price')) for r in pair]\n",
    "    \n",
    "    return [\n",
    "        normalized_jaccard(*names),\n",
    "        normalized_jaccard(*descr),\n",
    "        normalized_jaccard(*manuf),\n",
    "        abs(1.0/(1+ (price[0] - price[1]))),\n",
    "    ]\n",
    "\n",
    "print similarity(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Pairwise Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.90\n",
    "WEIGHTS   = (0.6, 0.1, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = 0\n",
    "for azprod in amazon.values():\n",
    "    for googprod in google.values():\n",
    "        vector = similarity(azprod, googprod)\n",
    "        score  = sum(map(lambda v: v[0]*v[1], zip(WEIGHTS, vector)))\n",
    "        if score > THRESHOLD:\n",
    "            matches += 1\n",
    "            print \"{0:0.3f}: {1} {2}\".format(\n",
    "                    score, azprod['id'], googprod['id'].split(\"/\")[-1]\n",
    "            )\n",
    "\n",
    "print \"\\n{} matches discovered\".format(matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
